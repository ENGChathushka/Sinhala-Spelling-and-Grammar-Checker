{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OoPJznBr2cne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0260ed63-124a-4ae5-bad7-aff5da3f786f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KsVyrZvbua4",
        "outputId": "ab4991a3-408d-4e34-b52f-39e65b895e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/244.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Model\n"
      ],
      "metadata": {
        "id": "nfOetAhceQlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import difflib\n",
        "import docx\n",
        "import pickle  # To save and load the tokenizer\n",
        "\n",
        "# Load dataset from Excel file\n",
        "data_path = '/content/drive/MyDrive/AI/Project/Grammer_data_set.xlsx'\n",
        "data = pd.read_excel(data_path)\n",
        "\n",
        "# Rename the columns explicitly to avoid issues\n",
        "data.columns = ['Sentence', 'True_Sentence', 'Label']  # Assuming 'Label' is the target column\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(data, save_path=None):\n",
        "    # Tokenize text data\n",
        "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(data['Sentence'])\n",
        "    sequences = tokenizer.texts_to_sequences(data['Sentence'])\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = max(len(seq) for seq in sequences)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Save the tokenizer if a save path is provided\n",
        "    if save_path:\n",
        "        with open(save_path, 'wb') as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return padded_sequences, tokenizer, max_len\n",
        "\n",
        "# Function to load Sinhala dictionary from a Word file\n",
        "def load_dictionary(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return [paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
        "\n",
        "# Load Sinhala words\n",
        "word_file_path = '/content/drive/MyDrive/AI/Project/Spell_correction_data.docx'\n",
        "dictionary = load_dictionary(word_file_path)\n",
        "\n",
        "# Spell Checking Functions\n",
        "def detect_errors(paragraph, dictionary):\n",
        "    words = paragraph.split()  # Split the paragraph into words\n",
        "    misspelled = [word for word in words if word not in dictionary]\n",
        "    return misspelled\n",
        "\n",
        "def suggest_correction(word, dictionary):\n",
        "    closest_match = difflib.get_close_matches(word, dictionary, n=1)\n",
        "    return closest_match[0] if closest_match else word  # Suggest closest or return original\n",
        "\n",
        "def correct_paragraph(paragraph, dictionary):\n",
        "    words = paragraph.split()\n",
        "    corrected = [\n",
        "        suggest_correction(word, dictionary) if word not in dictionary else word\n",
        "        for word in words\n",
        "    ]\n",
        "    return \" \".join(corrected)\n",
        "\n",
        "# Save path for the tokenizer\n",
        "tokenizer_save_path = '/content/drive/MyDrive/AI/Project/tokenizer.pkl'\n",
        "\n",
        "# Process data\n",
        "X, tokenizer, max_len = preprocess_data(data, save_path=tokenizer_save_path)\n",
        "\n",
        "# Convert labels to one-hot encoding for multi-class classification\n",
        "y = to_categorical(data['Label'].values, num_classes=3)  # Adjust number of classes\n",
        "\n",
        "# Split data into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')  # Adjusted for 3 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Save Model\n",
        "model_save_path = '/content/drive/MyDrive/AI/Project/results/sinhala_spell_grammar_model.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "# Function to check grammar and spelling\n",
        "def check_sentence(sentence, dictionary):\n",
        "    # Step 1: Spell check\n",
        "    misspelled_words = detect_errors(sentence, dictionary)\n",
        "    corrected_sentence = correct_paragraph(sentence, dictionary)\n",
        "\n",
        "    # Step 2: Grammar check using the model\n",
        "    sequence = tokenizer.texts_to_sequences([corrected_sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "    # Step 3: Provide feedback\n",
        "    feedback = []\n",
        "    if len(misspelled_words) > 0:\n",
        "        feedback.append(f\"Spelling mistakes detected: {', '.join(misspelled_words)}. Corrected sentence: {corrected_sentence}\")\n",
        "    feedback.append(f\"The predicted class for the sentence is: {predicted_class}\")\n",
        "\n",
        "    return \" \".join(feedback)\n",
        "\n",
        "# Evaluate on the 20% test set\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print classification report for evaluation\n",
        "print(\"Evaluation on 20% test set:\")\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBWwAO2qWHsl",
        "outputId": "2f62fd2b-0d13-45d4-83ba-78bcae2cf152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.6203 - loss: 1.0197 - val_accuracy: 0.6677 - val_loss: 0.6696\n",
            "Epoch 2/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6312 - loss: 0.6879 - val_accuracy: 0.6677 - val_loss: 0.5389\n",
            "Epoch 3/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7303 - loss: 0.5132 - val_accuracy: 0.8544 - val_loss: 0.4439\n",
            "Epoch 4/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8350 - loss: 0.4297 - val_accuracy: 0.9114 - val_loss: 0.2879\n",
            "Epoch 5/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8989 - loss: 0.2826 - val_accuracy: 0.9241 - val_loss: 0.1716\n",
            "Epoch 6/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8915 - loss: 0.2220 - val_accuracy: 0.9241 - val_loss: 0.1655\n",
            "Epoch 7/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9231 - loss: 0.1919 - val_accuracy: 0.9241 - val_loss: 0.1401\n",
            "Epoch 8/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8997 - loss: 0.1937 - val_accuracy: 0.9241 - val_loss: 0.1589\n",
            "Epoch 9/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9233 - loss: 0.1795 - val_accuracy: 0.9241 - val_loss: 0.1286\n",
            "Epoch 10/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9315 - loss: 0.1563 - val_accuracy: 0.9335 - val_loss: 0.1340\n",
            "Epoch 11/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9292 - loss: 0.1439 - val_accuracy: 0.9241 - val_loss: 0.1263\n",
            "Epoch 12/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9015 - loss: 0.1625 - val_accuracy: 0.9209 - val_loss: 0.1396\n",
            "Epoch 13/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9141 - loss: 0.1471 - val_accuracy: 0.9177 - val_loss: 0.1610\n",
            "Epoch 14/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9231 - loss: 0.1308 - val_accuracy: 0.9019 - val_loss: 0.2247\n",
            "Epoch 15/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9394 - loss: 0.1276 - val_accuracy: 0.9177 - val_loss: 0.1405\n",
            "Epoch 16/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9220 - loss: 0.1224 - val_accuracy: 0.9082 - val_loss: 0.2011\n",
            "Epoch 17/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9100 - loss: 0.1474 - val_accuracy: 0.9430 - val_loss: 0.2073\n",
            "Epoch 18/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9515 - loss: 0.1073 - val_accuracy: 0.9462 - val_loss: 0.1739\n",
            "Epoch 19/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9582 - loss: 0.0854 - val_accuracy: 0.9715 - val_loss: 0.1648\n",
            "Epoch 20/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9750 - loss: 0.0682 - val_accuracy: 0.9589 - val_loss: 0.1638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
            "Evaluation on 20% test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       211\n",
            "           1       0.95      0.92      0.94       105\n",
            "\n",
            "    accuracy                           0.96       316\n",
            "   macro avg       0.96      0.95      0.95       316\n",
            "weighted avg       0.96      0.96      0.96       316\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import docx\n",
        "import difflib\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load Sinhala dictionary from a Word file\n",
        "def load_dictionary(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return [paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
        "\n",
        "# Spell Checking Functions\n",
        "def detect_errors(words, dictionary):\n",
        "    return [word for word in words if word not in dictionary]\n",
        "\n",
        "def suggest_correction(word, dictionary):\n",
        "    closest_match = difflib.get_close_matches(word, dictionary, n=1)\n",
        "    return closest_match[0] if closest_match else word\n",
        "\n",
        "def correct_paragraph(sentence, dictionary):\n",
        "    words = sentence.split()\n",
        "    if len(words) <= 2:  # Handle short sentences separately\n",
        "        return sentence\n",
        "    corrected = [\n",
        "        suggest_correction(word, dictionary) if word not in dictionary else word\n",
        "        for word in words[1:-1]  # Check only the middle words for spelling\n",
        "    ]\n",
        "    return \" \".join([words[0]] + corrected + [words[-1]])  # Reassemble with first and last word intact\n",
        "\n",
        "\n",
        "# Define the max_len based on your model training\n",
        "max_len = 30  # Ensure this matches the value used during training\n",
        "\n",
        "# Grammar Checking Based on Rules\n",
        "def check_grammar(sentence):\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Rule 1: Starts with 'මම' -> Ends with 'මි'\n",
        "    if words[0] == \"මම\":\n",
        "        if not words[-1].endswith(\"මි\"):\n",
        "            return False\n",
        "\n",
        "    # Rule 2: Starts with 'අපි' -> Ends with 'මු'\n",
        "    elif words[0] == \"අපි\":\n",
        "        if not words[-1].endswith(\"මු\"):\n",
        "            return False\n",
        "\n",
        "    # Rule 3: Starts with any word -> Ends with 'යි'\n",
        "    else:\n",
        "        if not words[-1].endswith(\"යි\"):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Function to check grammar and spelling\n",
        "def check_sentence(sentence, dictionary, model, tokenizer, max_len):\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Step 1: Spell check middle words\n",
        "    middle_words = words[1:-1]\n",
        "    misspelled_words = detect_errors(middle_words, dictionary)\n",
        "    corrected_sentence = correct_paragraph(sentence, dictionary)\n",
        "\n",
        "    # Step 2: Grammar check using the trained model\n",
        "    sequence = tokenizer.texts_to_sequences([corrected_sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    is_grammar_correct = prediction[0][0] > 0.5\n",
        "\n",
        "    # Step 3: Provide feedback\n",
        "    feedback = []\n",
        "    if misspelled_words:\n",
        "        feedback.append(f\"Spelling mistakes detected: {', '.join(misspelled_words)}.\")\n",
        "        feedback.append(f\"Corrected sentence: {corrected_sentence}\")\n",
        "    else:\n",
        "        feedback.append(\"No spelling mistakes detected.\")\n",
        "\n",
        "    if not is_grammar_correct:\n",
        "        feedback.append(\"The sentence is grammatically incorrect according to the model.\")\n",
        "    else:\n",
        "        feedback.append(\"The sentence is grammatically correct.\")\n",
        "\n",
        "    return \"\\n\".join(feedback)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/AI/Project/results/sinhala_spell_grammar_model.h5')\n",
        "\n",
        "# Compile the model after loading it to remove the warning\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load the tokenizer (ensure the path is correct)\n",
        "tokenizer_save_path = '/content/drive/MyDrive/AI/Project/tokenizer.pkl'\n",
        "with open(tokenizer_save_path, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load Sinhala words dictionary\n",
        "word_file_path = '/content/drive/MyDrive/AI/Project/Spell_correction_data.docx'\n",
        "dictionary = load_dictionary(word_file_path)\n",
        "\n",
        "# Interactive Function for Sentence Checking\n",
        "def check_multiple_sentences():\n",
        "    print(\"Enter sentences to check for spelling and grammar. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "        if sentence.lower() == 'exit':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "\n",
        "        result = check_sentence(sentence, dictionary, model, tokenizer, max_len)\n",
        "        print(f\"Sentence: '{sentence}'\\nResult:\\n{result}\\n\")\n",
        "\n",
        "# Run the function to check sentences\n",
        "check_multiple_sentences()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USCrDv-kfTuL",
        "outputId": "53b019cc-2bf6-4d03-a7e1-0a8b19bcb3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter sentences to check for spelling and grammar. Type 'exit' to stop.\n",
            "Enter a sentence: තාත්තා ගුවන්විදුලියට සවන්දෙයි\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step\n",
            "Sentence: 'තාත්තා ගුවන්විදුලියට සවන්දෙයි'\n",
            "Result:\n",
            "No spelling mistakes detected.\n",
            "The sentence is grammatically correct.\n",
            "\n",
            "Enter a sentence: ඇය ඇපල් කමු\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Sentence: 'ඇය ඇපල් කමු'\n",
            "Result:\n",
            "No spelling mistakes detected.\n",
            "The sentence is grammatically correct.\n",
            "\n",
            "Enter a sentence: මම කොත්තු කමි\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Sentence: 'මම කොත්තු කමි'\n",
            "Result:\n",
            "No spelling mistakes detected.\n",
            "The sentence is grammatically incorrect according to the model.\n",
            "\n",
            "Enter a sentence: මම කොත්තු කමු\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Sentence: 'මම කොත්තු කමු'\n",
            "Result:\n",
            "No spelling mistakes detected.\n",
            "The sentence is grammatically incorrect according to the model.\n",
            "\n",
            "Enter a sentence: exit\n",
            "Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new\n"
      ],
      "metadata": {
        "id": "5G9cuKrzqmIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import docx\n",
        "import difflib\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load Sinhala dictionary from a Word file\n",
        "def load_dictionary(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return [paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
        "\n",
        "# Spell Checking Functions\n",
        "def detect_errors(words, dictionary):\n",
        "    return [word for word in words if word not in dictionary]\n",
        "\n",
        "def suggest_correction(word, dictionary):\n",
        "    closest_match = difflib.get_close_matches(word, dictionary, n=1)\n",
        "    return closest_match[0] if closest_match else word\n",
        "\n",
        "def correct_paragraph(sentence, dictionary):\n",
        "    words = sentence.split()\n",
        "    if len(words) <= 2:  # Handle short sentences separately\n",
        "        return sentence\n",
        "    corrected = [\n",
        "        suggest_correction(word, dictionary) if word not in dictionary else word\n",
        "        for word in words[1:-1]  # Check only the middle words for spelling\n",
        "    ]\n",
        "    return \" \".join([words[0]] + corrected + [words[-1]])  # Reassemble with first and last word intact\n",
        "\n",
        "# Define the max_len based on your model training\n",
        "max_len = 30  # Ensure this matches the value used during training\n",
        "\n",
        "# Grammar Checking Based on Rules\n",
        "def check_grammar(sentence):\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Rule 1: Starts with 'මම' -> Ends with 'මි'\n",
        "    if words[0] == \"මම\":\n",
        "        if not words[-1].endswith(\"මි\"):\n",
        "            corrected_sentence = sentence.rstrip(words[-1]) + \"මි\"\n",
        "            return False, corrected_sentence\n",
        "\n",
        "    # Rule 2: Starts with 'අපි' -> Ends with 'මු'\n",
        "    elif words[0] == \"අපි\":\n",
        "        if not words[-1].endswith(\"මු\"):\n",
        "            corrected_sentence = sentence.rstrip(words[-1]) + \"මු\"\n",
        "            return False, corrected_sentence\n",
        "\n",
        "    # Rule 3: Starts with any word -> Ends with 'යි'\n",
        "    else:\n",
        "        if not words[-1].endswith(\"යි\"):\n",
        "            corrected_sentence = sentence.rstrip(words[-1]) + \"යි\"\n",
        "            return False, corrected_sentence\n",
        "\n",
        "    return True, sentence\n",
        "\n",
        "# Function to check grammar and spelling\n",
        "def check_sentence(sentence, dictionary, model, tokenizer, max_len, grammar_data):\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Step 1: Spell check middle words\n",
        "    middle_words = words[1:-1]\n",
        "    misspelled_words = detect_errors(middle_words, dictionary)\n",
        "    corrected_sentence = correct_paragraph(sentence, dictionary)\n",
        "\n",
        "    # Step 2: Grammar check using the trained model\n",
        "    sequence = tokenizer.texts_to_sequences([corrected_sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    is_grammar_correct = prediction[0][0] > 0.5\n",
        "\n",
        "    # Step 3: Grammar rule check and correction\n",
        "    grammar_is_correct, grammar_corrected_sentence = check_grammar(corrected_sentence)\n",
        "\n",
        "    # Step 4: Provide feedback\n",
        "    feedback = []\n",
        "    if misspelled_words:\n",
        "        feedback.append(f\"Spelling mistakes detected: {', '.join(misspelled_words)}.\")\n",
        "        feedback.append(f\"Corrected sentence: {corrected_sentence}\")\n",
        "    else:\n",
        "        feedback.append(\"No spelling mistakes detected.\")\n",
        "\n",
        "    if not grammar_is_correct:\n",
        "        feedback.append(f\"The sentence is grammatically incorrect according to the model.\")\n",
        "    else:\n",
        "        feedback.append(\"The sentence is grammatically correct.\")\n",
        "\n",
        "    # Step 5: Map incorrect sentence to the correct sentence using the grammar dataset\n",
        "    incorrect_sentence = grammar_data.get(sentence)\n",
        "    if incorrect_sentence:\n",
        "        feedback.append(f\"Corrected Sentence: {incorrect_sentence}\")\n",
        "\n",
        "    return \"\\n\".join(feedback)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/AI/Project/results/sinhala_spell_grammar_model.h5')\n",
        "\n",
        "# Compile the model after loading it to remove the warning\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load the tokenizer (ensure the path is correct)\n",
        "tokenizer_save_path = '/content/drive/MyDrive/AI/Project/tokenizer.pkl'\n",
        "with open(tokenizer_save_path, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load Sinhala words dictionary\n",
        "word_file_path = '/content/drive/MyDrive/AI/Project/Spell_correction_data.docx'\n",
        "dictionary = load_dictionary(word_file_path)\n",
        "\n",
        "# Load Grammar correction dataset\n",
        "grammar_data_path = '/content/drive/MyDrive/AI/Project/Grammer_data_set.xlsx'\n",
        "df = pd.read_excel(grammar_data_path)\n",
        "\n",
        "# Check the column names to verify what they are\n",
        "print(\"Column names in the dataset:\", df.columns)\n",
        "\n",
        "# Assuming the columns are 'Sentence' and 'True Sentence'\n",
        "# Map the incorrect sentence to the correct sentence in a dictionary for fast lookup\n",
        "grammar_data = dict(zip(df['Sentence'], df['True Sentence']))\n",
        "\n",
        "# Interactive Function for Sentence Checking\n",
        "def check_multiple_sentences():\n",
        "    print(\"Enter sentences to check for spelling and grammar. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "        if sentence.lower() == 'exit':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "\n",
        "        result = check_sentence(sentence, dictionary, model, tokenizer, max_len, grammar_data)\n",
        "        print(f\"Sentence: '{sentence}'\\nResult:\\n{result}\\n\")\n",
        "\n",
        "# Run the function to check sentences\n",
        "check_multiple_sentences()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uipa2iyAqnYb",
        "outputId": "be7a8caa-874d-46f9-8719-914fe99c3142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the dataset: Index(['Sentence', 'True Sentence', True], dtype='object')\n",
            "Enter sentences to check for spelling and grammar. Type 'exit' to stop.\n",
            "Enter a sentence: අපි පලතුරු කයි\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step\n",
            "Sentence: 'අපි පලතුරු කයි'\n",
            "Result:\n",
            "No spelling mistakes detected.\n",
            "The sentence is grammatically incorrect according to the model.\n",
            "Corrected Sentence: අපි පලතුරු කමු\n",
            "\n",
            "Enter a sentence: අපි ගීත කියමි\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Sentence: 'අපි ගීත කියමි'\n",
            "Result:\n",
            "No spelling mistakes detected.\n",
            "The sentence is grammatically incorrect according to the model.\n",
            "Corrected Sentence: අපි ගීත කියමු\n",
            "\n",
            "Enter a sentence: මම පන්සල යමි\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Sentence: 'මම පන්සල යමි'\n",
            "Result:\n",
            "Spelling mistakes detected: පන්සල.\n",
            "Corrected sentence: මම පන්සල් යමි\n",
            "The sentence is grammatically correct.\n",
            "\n",
            "Enter a sentence: නංගී සාරිය අඳිමු\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Sentence: 'නංගී සාරිය අඳිමු'\n",
            "Result:\n",
            "Spelling mistakes detected: සාරිය.\n",
            "Corrected sentence: නංගී සාර අඳිමු\n",
            "The sentence is grammatically incorrect according to the model.\n",
            "Corrected Sentence: නංගී සාරිය අඳියි\n",
            "\n",
            "Enter a sentence: exit\n",
            "Exiting...\n"
          ]
        }
      ]
    }
  ]
}